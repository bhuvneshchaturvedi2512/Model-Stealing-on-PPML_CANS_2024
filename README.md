This repository provides the code that has been used to obtain the results for our [CANS 2024 paper](https://link.springer.com/chapter/10.1007/978-981-97-8016-7_5). More specifically, we report the accuracy of the stolen model when using an unprotected model vs a model protected by our proposed countermeasure. We carried out two well known attacks, namely Jacobian-based Dataset Augmentation (JBDA) and Knockoff Nets, on two models trained on the MNIST and CIFAR-10 dataset. The results are obtained on a system running on Intel Xeon Silver 4210R @ 2.4 GHz with 128 GB RAM, 27.5 MB LLC and powered by Ubuntu 20.04 LTS. The results obtained via execution of the provided scripts should be similar to the results quoted in our paper. For BGV, the results will differ due to the usage of a different set of parameters than the one used in the paper.

Before proceeding with the demo code, please run `sudo bash setup.sh` to install the prerequisite libraries. The prerequisites include python3 and concrete-ml. Please skip this setup step if these tools are already installed on the testing system. We have used python 3.8.10 and concrete-ml 1.3.0, so anything above these should also work. Optionally, one can install CUDA on the system. While the scripts will work even without CUDA support, having the same will allow the usage of GPU in place of CPU to speed up the execution.

Automating the scripts to carry out the attacks will be a time consuming affair, and unfortunately do not have sufficient bandwidth to allocate to this task. But I will do so eventually. Till then, stay tuned for any updates.

For any queries, please feel free to email them to us on [bhuvneshchaturvedi2512@kgpian.iitkgp.ac.in](mailto:bhuvneshchaturvedi2512@kgpian.iitkgp.ac.in).
